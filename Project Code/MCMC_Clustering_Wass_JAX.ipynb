{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "#Separate import for numpy and jax.numpy to have major felxibility\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit,lax,random\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow_probability.substrates import jax as jtfp\n",
    "jtfd = jtfp.distributions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_perturbed = np.load('test4.npy')\n",
    "data = sampled_perturbed[0 : 1000]\n",
    "#DEFINING NUMBER OF CLUSTERS AND DIMENSION OF DATA\n",
    "k = 15\n",
    "d = 2\n",
    "#Key to store for random generation\n",
    "rng_key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def g(x):\n",
    "    \"\"\"\n",
    "    Diggle Graton Modulation Function\n",
    "    \"\"\"\n",
    "    #Shape parameters\n",
    "    r = 5\n",
    "    gamma = 0.2\n",
    "    asymptote = 10\n",
    "    logic = (x <= r)\n",
    "    return (logic* jnp.power(x/r,1/gamma) + (1 - logic) )*asymptote\n",
    "\n",
    "@jit\n",
    "def sample_uniq_vals_fullcond_Wasserstein_NW_log(data,cluster_allocs, mus,covs, h,bij,cov_prop_mu,cov_prop_cov,key):\n",
    "    \"\"\"\n",
    "    Perform one step of the Metropolis Hastings to sample a couple (mu,cov) from N*IW*Wasserstein\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Starting point of MH\n",
    "    mu_old = mus[h]\n",
    "    cov_old = covs[h]\n",
    "    log_jac_cov_old = abs(bij.forward_log_det_jacobian( bij.inverse(cov_old) ) )\n",
    "    \n",
    "    # Sample the proposal using Bijector and compute jacobian term of transformation\n",
    "    mu, cov, log_jac_cov,key = sample_from_bijector(mu_old, cov_prop_mu, cov_old, cov_prop_cov,bij,key)\n",
    "    jac_term = log_jac_cov - log_jac_cov_old\n",
    "    \n",
    "    # Compute acceptance rate(and return it for tuning)\n",
    "    beta = compute_beta_NW_log(data,cluster_allocs,mus,covs, h, mu, cov,mu_old, cov_old,jac_term)\n",
    "    beta = jnp.minimum(0, beta)\n",
    "    accept_rate = jnp.exp(beta)\n",
    "    \n",
    "    key,subk = random.split(key)\n",
    "    draw = jtfd.Uniform().sample(seed = subk)\n",
    "    \n",
    "    # Select the new or old values \n",
    "    logic = jnp.log(draw) >= beta\n",
    "    ret_mu = logic * jnp.array(mu_old) + (1 - logic)*jnp.array(mu)\n",
    "    ret_cov = logic * jnp.array(cov_old) + (1 - logic)*jnp.array(cov)\n",
    "\n",
    "    return ret_mu,ret_cov,accept_rate,key\n",
    "    \n",
    "    \n",
    "@jit\n",
    "def sample_uniq_vals_prior_Wasserstein_NW_log(mus,covs, h,bij,cov_prop_mu,cov_prop_cov,key):\n",
    "    \"\"\"\n",
    "    Perform one step of the Metropolis Hastings to sample a couple (mu,cov) from N*IW*Wasserstein\n",
    "    \"\"\"\n",
    "    \n",
    "    # Starting point of MH\n",
    "    mu_old = mus[h]\n",
    "    cov_old = covs[h]\n",
    "    log_jac_cov_old = abs(bij.forward_log_det_jacobian( bij.inverse(cov_old) ) )\n",
    "    \n",
    "    # Sample the proposal using Bijector and compute jacobian term of transformation\n",
    "    mu, cov, log_jac_cov,subk1 = sample_from_bijector(mu_old, cov_prop_mu, cov_old, cov_prop_cov,bij,key)\n",
    "    jac_term = log_jac_cov - log_jac_cov_old\n",
    "    \n",
    "    # Compute acceptance rate(and return it for tuning)\n",
    "    beta = compute_beta_NW_prior_log(mus,covs, h, mu, cov,mu_old, cov_old,jac_term)\n",
    "    beta = jnp.minimum(0, beta)\n",
    "    accept_rate = jnp.exp(beta)\n",
    "    \n",
    "    key,subk = random.split(key)\n",
    "    draw = jtfd.Uniform().sample(seed = subk)\n",
    "    \n",
    "    # Select the new or old values \n",
    "    logic = jnp.log(draw) >= beta\n",
    "    ret_mu = logic * jnp.array(mu_old) + (1 - logic)*jnp.array(mu)\n",
    "    ret_cov = logic * jnp.array(cov_old) + (1 - logic)*jnp.array(cov)\n",
    "\n",
    "    return ret_mu,ret_cov,accept_rate,key\n",
    "    \n",
    "\n",
    "@jit\n",
    "def sample_from_bijector(mu_old, cov_prop_mu, cov_old, cov_prop_cov, bijector,key):\n",
    "    \"\"\"\n",
    "    Sample new mean and covariance matrix through the bijection\n",
    "    \"\"\"\n",
    "    key , subk1 , subk2 = random.split(key,3)\n",
    "\n",
    "    mix = 0.1\n",
    "    bimix_gauss = jtfd.MixtureSameFamily(\n",
    "        \n",
    "                     mixture_distribution=jtfd.Categorical(probs=[mix, 1-mix]),\n",
    "                     components_distribution=jtfd.MultivariateNormalDiag(loc=[mu_old,mu_old],scale_diag=[[3,3],[0.1,0.1]])\n",
    "                   )  \n",
    "\n",
    "    mu = bimix_gauss.sample(seed = subk1)\n",
    "    \n",
    "    chol = jnp.linalg.cholesky(cov_old)\n",
    "    vec = bijector.inverse(chol)\n",
    "    vec_sampl = jtfd.MultivariateNormalFullCovariance(vec,cov_prop_cov).sample(seed = subk2)\n",
    "    cov_sampl = bijector.forward(vec_sampl)\n",
    "    cov = jnp.matmul(cov_sampl , jnp.transpose(cov_sampl))\n",
    "    log_jac = abs(bijector.forward_log_det_jacobian(vec_sampl))\n",
    "    \n",
    "    return mu,cov,log_jac,key\n",
    "\n",
    "@jit\n",
    "def compute_beta_NW_log(data,cluster_allocs,mus,covs, h, mu, cov,mu_old, cov_old,jac_term):\n",
    "    \"\"\"\n",
    "    Compute Metropolis Hastings term, knowing the proposal is symmetric\n",
    "    The target is the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_n\n",
    "    \"\"\"\n",
    "    cluster_allocs_expanded = jnp.expand_dims(cluster_allocs,axis = 1)\n",
    "    nan_vec = jnp.repeat(jnp.array([[jnp.nan,jnp.nan]]),len(cluster_allocs),axis=0)\n",
    "    clusdata = jnp.where(jnp.concatenate((cluster_allocs_expanded,cluster_allocs_expanded),axis = 1) == h,data,nan_vec)\n",
    "       \n",
    "    mu_0 = jnp.zeros(2)\n",
    "    cov_0 = jnp.eye(2)\n",
    "    nu_0 = 5\n",
    "    phi_0 = jnp.eye(2)*(nu_0 - 3)\n",
    "    \n",
    "    # NW distribution terms\n",
    "    num_0 = evaluate_NW_log(mu, cov, mu_0, cov_0, phi_0, nu_0)\n",
    "    den_0 = evaluate_NW_log(mu_old, cov_old , mu_0, cov_0, phi_0, nu_0)\n",
    "    \n",
    "    # Data distribution terms\n",
    "    num_1 = jnp.nansum(jtfd.MultivariateNormalFullCovariance(mu, cov).log_prob(clusdata))\n",
    "    den_1 = jnp.nansum(jtfd.MultivariateNormalFullCovariance(mu_old, cov_old).log_prob(clusdata))\n",
    "    \n",
    "    # Wasserstein Distance terms\n",
    "    wass_arr = jnp.array([jnp.log(compute_Wasserstein(mu , cov , mus[j] , covs[j])) - jnp.log(compute_Wasserstein(mu_old , cov_old , mus[j], covs[j])) for j in range(k)])\n",
    "    cond_arr = jnp.arange(k)\n",
    "    nan_arr = jnp.repeat(jnp.nan,k)\n",
    "    sum_ = jnp.nansum(jnp.where(cond_arr != h,wass_arr,nan_arr))\n",
    "\n",
    "    return num_0 + num_1  - den_0 - den_1 + sum_ + jac_term\n",
    "\n",
    "@jit\n",
    "def compute_beta_NW_prior_log(mus,covs, h, mu, cov,mu_old, cov_old,jac_term):\n",
    "    \"\"\"\n",
    "    Compute Metropolis Hastings term, knowing the proposal is symmetric\n",
    "    The target is the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nus the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_\"s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nus the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_\"n\n",
    "    \"\"\"\n",
    "    \n",
    "    mu_0 = jnp.zeros(2)\n",
    "    cov_0 = jnp.eye(2)\n",
    "    nu_0 = 5\n",
    "    phi_0 = jnp.eye(2)*(nu_0 - 3)\n",
    "    \n",
    "    # NW distribution terms\n",
    "    num = evaluate_NW_log(mu, cov, mu_0, cov_0, phi_0, nu_0)\n",
    "    den = evaluate_NW_log(mu_old, cov_old , mu_0, cov_0, phi_0, nu_0)\n",
    "    \n",
    "    # Wasserstein Distance terms\n",
    "    wass_arr = jnp.array([jnp.log(compute_Wasserstein(mu , cov , mus[j] , covs[j])) - jnp.log(compute_Wasserstein(mu_old , cov_old , mus[j], covs[j])) for j in range(k)])\n",
    "    cond_arr = jnp.arange(k)\n",
    "    nan_arr = jnp.repeat(jnp.nan,k)\n",
    "    sum_ = jnp.nansum(jnp.where(cond_arr != h,wass_arr,nan_arr))\n",
    "    \n",
    "    return num  - den + sum_ + jac_term\n",
    "  \n",
    "@jit\n",
    "def evaluate_NW_log(x_mu, x_cov, mu_n, cov_n, phi_n, nu_n):\n",
    "    \"\"\"\n",
    "    Evaluate the likelihood: L[ (x_mu, x_cov) | ( mu_n, cov_n, phi_n, nu_n ) ]\n",
    "    L = Normal x Inverse Wishart\n",
    "    \"\"\"\n",
    "    W = jnp.linalg.inv(phi_n)\n",
    "    chol = jnp.linalg.cholesky(W) \n",
    "    inv_cov = jnp.linalg.inv(x_cov)\n",
    "\n",
    "    f1 = jtfd.WishartTriL(df=nu_n.astype(float), scale_tril=chol).log_prob(inv_cov)\n",
    "    f2 = jtfd.MultivariateNormalFullCovariance(mu_n, cov_n).log_prob(x_mu)\n",
    "    \n",
    "    return f1 + f2\n",
    "\n",
    "@jit\n",
    "def compute_Wasserstein(mu_1, cov_1, mu_2, cov_2):\n",
    "    \"\"\"\n",
    "    Wasserstein distance for the Gaussian Case, already taking consideration of g() to bound Wasserstein term\n",
    "    \"\"\"\n",
    "    norm = jnp.linalg.norm(mu_1 - mu_2, ord = 2)\n",
    "    \n",
    "    #Compute square root of cov_2\n",
    "    s = jnp.sqrt(jnp.linalg.det(cov_2))\n",
    "    t = jnp.sqrt(jnp.trace(cov_2) + 2*s)\n",
    "    sqrt_C2 = 1/t * (cov_2 + s*jnp.eye(2))\n",
    "    \n",
    "    C1_sqrt_C2 = jnp.matmul(cov_1,sqrt_C2)\n",
    "    sqrt_C2_C1_sqrt_C2 = jnp.matmul(sqrt_C2,C1_sqrt_C2)\n",
    "    \n",
    "    #Compute square root of sqrt_C2_C1_sqrt_C2\n",
    "    s = jnp.sqrt(jnp.linalg.det(sqrt_C2_C1_sqrt_C2))\n",
    "    t = jnp.sqrt(jnp.trace(sqrt_C2_C1_sqrt_C2) + 2*s)\n",
    "    mat_sqrt = 1/t * (sqrt_C2_C1_sqrt_C2 + s*jnp.eye(2))\n",
    "    \n",
    "    trace = jnp.trace(cov_1 + cov_2 - 2 * mat_sqrt)\n",
    "\n",
    "    # Function g\n",
    "    return g(norm+trace)\n",
    "\n",
    "@jit\n",
    "def update_weights(cluster_allocs, weights, k, alpha,key):\n",
    "\n",
    "    key, subk = random.split(key)\n",
    "    n_clus = len(weights)\n",
    "    n_by_clus = jnp.array([jnp.sum(cluster_allocs == h) for h in range(n_clus)])\n",
    "    post_params = jnp.ones(n_clus) * alpha + n_by_clus\n",
    "    return jtfd.Dirichlet(post_params.astype(float)).sample(seed = subk),key\n",
    "\n",
    "@jit\n",
    "def update_cluster_allocs(data, weights, mus, covs,key):\n",
    "    #print(weights)\n",
    "    key, subk = random.split(key)\n",
    "    logprobs = jtfd.MultivariateNormalFullCovariance(mus, covs).log_prob(data[:, jnp.newaxis])\n",
    "    logprobs += jnp.log(weights)\n",
    "    probs =  jnp.exp(logprobs)/(jnp.sum(jnp.exp(logprobs), axis=1))[:,None]\n",
    "    return jtfd.Categorical(probs=probs, validate_args=True).sample(seed = subk),key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_gibbs(data, cluster_allocs, mus,covs, weights, alpha,bij,cov_prop_mu,cov_prop_cov,key):\n",
    "    \"\"\"\n",
    "    Run one gibbs sampler iteration\n",
    "    Takes in input values of the previous iteration and sample the new values from sample_uniq_vals_fullcond, update_weights and update_cluster_allocs\n",
    "    Returns:   \n",
    "    \n",
    "    -cluster_allocs: for every data point, the cluster assigned\n",
    "    -uniq_vals: array of parameters of the distributions. Matrix has d rows of the type:[mu[0], var[0,0], var[0,1]]\n",
    "    -weights: array with the weights of the clusters \n",
    "     \"\"\"\n",
    "    key, subk = random.split(key)\n",
    "    n_clus = len(weights)\n",
    "    for h in range(n_clus):\n",
    "        \n",
    "        #Extract data assigned to cluster h and sample \n",
    "        clusdata = data[cluster_allocs == h]\n",
    "        if(len(clusdata) != 0):\n",
    "            mus[h],covs[h], acc_rate,key = sample_uniq_vals_fullcond_Wasserstein_NW_log(data,cluster_allocs, mus,covs, h,bij,cov_prop_mu,cov_prop_cov,key)\n",
    "        else:\n",
    "            mus[h],covs[h], acc_rate,key = sample_uniq_vals_prior_Wasserstein_NW_log(mus,covs, h,bij,cov_prop_mu,cov_prop_cov,key)\n",
    "\n",
    "\n",
    "    weights,key = update_weights(cluster_allocs ,weights, k, alpha,key)\n",
    "\n",
    "    cluster_allocs,key = update_cluster_allocs(data, weights, mus,covs,key)\n",
    "\n",
    "    return cluster_allocs, mus,covs, weights, acc_rate,key\n",
    "\n",
    "def run_mcmc(data, k, key , niter=1000, nburn=300, thin=5 ): \n",
    "    \"\"\"\n",
    "    Runs entire MCMC\n",
    "    Takes in input data, number of clusters, number of iterations, burn-in and thin\n",
    "    Returns the parameters recorded after burn-in phase\n",
    "    \"\"\"\n",
    "\n",
    "    b = time.time() # only to measure time\n",
    "    \n",
    "    #Bijector setup\n",
    "    jtfb = jtfp.bijectors\n",
    "    bij = jtfb.Chain([\n",
    "            jtfb.TransformDiagonal(jtfb.Softplus()),\n",
    "            jtfb.FillTriangular()])\n",
    "\n",
    "    #Starting Values\n",
    "    cluster_allocs = tfd.Categorical(probs=np.ones(k) / k).sample(len(data))\n",
    "    weights = jnp.ones(k)/k\n",
    "    alpha = 0.1\n",
    "    lam = 0.1\n",
    "    mus = np.array(jtfd.MultivariateNormalFullCovariance(np.mean(np.array(data), axis=0), np.eye(2) ).sample(k,seed = key))\n",
    "    covs = np.array(jtfd.WishartTriL(df=d, scale_tril=tf.linalg.cholesky(np.eye(2)) ).sample(k,seed = key))\n",
    "    \n",
    "    #Proposal Covariances\n",
    "    cov_prop_cov = 0.02 * jnp.eye(3) #covariance matrix for the covariance sampling\n",
    "    cov_prop_mu = 0.1 * jnp.eye(2)   #covariance matrix for the mean sampling\n",
    "    \n",
    "    #Output values\n",
    "    allocs_out = []\n",
    "    mus_out = []\n",
    "    covs_out = []\n",
    "    weights_out = []\n",
    "    acc_rates = []\n",
    "    \n",
    "    #Useful value\n",
    "    data_mean = np.mean(data, axis = 0)\n",
    "    \n",
    "    #Iterations\n",
    "    for i in range(niter):\n",
    "        cluster_allocs, mus,covs, weights, acc_rate,key = run_one_gibbs(\n",
    "            data, cluster_allocs, mus,covs, weights, alpha,bij,cov_prop_mu,cov_prop_cov,key)\n",
    "        acc_rates.append(acc_rate)\n",
    "        \n",
    "        if i > nburn and i % thin == 0:\n",
    "            allocs_out.append(cluster_allocs)\n",
    "            mus_out.append(mus.copy())\n",
    "            covs_out.append(covs.copy())\n",
    "            weights_out.append(weights)\n",
    "            \n",
    "        if i % 10 == 0 and i > 11:\n",
    "            a = time.time()\n",
    "            print(\"\\rIter {0} / {1}\".format(i+1, niter) + \" Remaining minutes: \" + str(round((a-b)*(niter-i)/(60*10) ,1)) , flush=False, end=\" \")\n",
    "            b = time.time()\n",
    "            print(\"Acceptance rate: \" + str(np.mean(acc_rates[-10:])))\n",
    "            \n",
    "    return allocs_out, mus_out,covs_out, weights_out,acc_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNING THE MCMC\n",
    "start = time.time()\n",
    "allocs_out, mus_out,covs_out, weights_out,accept_rate_out = run_mcmc(data, k,rng_key , niter = 4000,nburn = 1000,thin = 5)\n",
    "end = time.time()\n",
    "print(\"Total time: \" + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binder Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = []\n",
    "for i in range(len(allocs_out)):\n",
    "    yy.append(len(np.unique(allocs_out[i])))\n",
    "\n",
    "xx,cc = np.unique(yy,return_counts=True)\n",
    "print(xx,cc)\n",
    "plt.bar(xx,cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best clustering obtained during the MCMC using the Binder Loss \n",
    "\n",
    "def get_psm(clus_alloc_chain):\n",
    "    \"\"\"\n",
    "    Returns the posterior similarity matrix, i.e.\n",
    "        out[i, j] = P(c_i == c_j | all the rest)\n",
    "    for each pair of observations\n",
    "    \"\"\"\n",
    "    #Fare confronto tra colonna i e j della matrice e poi calcolare da li\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    out = np.zeros((c_chain.shape[1], c_chain.shape[1]))\n",
    "    for i in range(c_chain.shape[1]):\n",
    "        for j in range(c_chain.shape[1]):\n",
    "            out[i,j] = np.sum(c_chain[:,i] == c_chain[:,j]) / c_chain.shape[0]\n",
    "            \n",
    "    return out\n",
    "\n",
    "\n",
    "def minbinder_sample(clus_alloc_chain, psm):\n",
    "    \"\"\"\n",
    "    Finds the iteration for which the Binder loss is minimized\n",
    "    and returns the associated clustering\n",
    "    \"\"\"\n",
    "    losses = np.zeros(len(clus_alloc_chain))\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    \n",
    "    # You can either cycle through the iterations, or \n",
    "    # cycle through the entries in the PSM [i, j]\n",
    "    # and vectorize the same operation for each iteration!\n",
    "    \n",
    "    mat = np.zeros( ( c_chain.shape[0], c_chain.shape[1]**2 ) )\n",
    "    k = 0\n",
    "    for i in range(c_chain.shape[1]):\n",
    "        for j in range(c_chain.shape[1]):\n",
    "            mat[:,k] = ( (c_chain[:,i] == c_chain[:,j]).astype(int) - psm[i,j] * np.ones(c_chain.shape[0]) ) **2\n",
    "            k = k+1\n",
    "    \n",
    "    losses = np.sum(mat, axis = 1)\n",
    "    \n",
    "    best_iter = np.argmin(losses)\n",
    "    return clus_alloc_chain[best_iter]\n",
    "\n",
    "\n",
    "\n",
    "def unique_vals_given_clus(unique_vals_chain, clus_alloc_chain, best_clus,n_clus):\n",
    "    #Finds the unique values associated to the best clusters obtained during MCMC\n",
    "    \n",
    "    c_allocs = np.stack(clus_alloc_chain)\n",
    "    uniq_vals = np.stack(unique_vals_chain)\n",
    "    means = uniq_vals[:, :, : , 0]\n",
    "    variances = uniq_vals[:, :, : , 1:3]\n",
    "    out1 = []\n",
    "    out2 = []\n",
    "    for h in range(n_clus):\n",
    "        data_idx = np.where(best_clus == h)[0]\n",
    "        uniq_vals_idx = c_allocs[:, data_idx] # -> Matrix [n_iter x n_data_in_clus]\n",
    "        means_by_iter = np.empty((c_allocs.shape[0], len(data_idx),2))\n",
    "        vars_by_iter = np.empty((c_allocs.shape[0], len(data_idx),2,2))\n",
    "        for i in range(c_allocs.shape[0]):\n",
    "            means_by_iter[i, :] = means[i,uniq_vals_idx[i,:],:]\n",
    "            vars_by_iter[i, :] = variances[i,uniq_vals_idx[i,:]]\n",
    "\n",
    "        avg_mean_by_iter = np.mean(means_by_iter , axis = 1)\n",
    "        avg_var_by_iter = np.mean(vars_by_iter , axis = 1)\n",
    "        \n",
    "        muhat = np.mean(avg_mean_by_iter,axis=0)\n",
    "        sigsqhat = np.mean(avg_var_by_iter,axis=0)\n",
    "        out1.append(np.array(muhat))\n",
    "        out2.append(np.array(sigsqhat))\n",
    "    \n",
    "    #If there are no points in certain clusters, the output would be full of NANs for the cluster. This gives problems\n",
    "    #I replace the NANs with some 0\n",
    "    for i in range(len(out1)):\n",
    "        if(np.any(np.isnan(out1[i]))):\n",
    "            out1[i] = np.zeros(shape = out1[i].shape)\n",
    "        if(np.any(np.isnan(out2[i]))):\n",
    "            out2[i] = np.eye(N = out2[i].shape[0])\n",
    "    \n",
    "        \n",
    "    return out1,out2\n",
    "\n",
    "\n",
    "psm = get_psm(allocs_out)\n",
    "best_clus = minbinder_sample(allocs_out,psm)\n",
    "\n",
    "uniq_vals_out = []\n",
    "for n in range(len(mus_out)):\n",
    "    mus = mus_out[n]\n",
    "    covs = covs_out[n]\n",
    "    arr = np.zeros(shape=(k,2,3))\n",
    "    for i in range(k):\n",
    "        mu = mus[i]\n",
    "        cov = covs[i]\n",
    "        arr[i,:,:] = np.array( [ [mu[0],cov[0,0],cov[0,1]] , [mu[1],cov[1,0],cov[1,1]] ])\n",
    "    uniq_vals_out.append(arr)\n",
    "    \n",
    "mu_lis,var_lis = unique_vals_given_clus(uniq_vals_out, allocs_out, best_clus,k)\n",
    "weights = np.sum(best_clus == np.arange(k)[:, np.newaxis], axis=1) / len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in range(k):\n",
    "    currd = data[best_clus == h]\n",
    "    sns.scatterplot(x=[elem[0] for elem in currd], y=[elem[1] for elem in currd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    \n",
    "    #Get mean of the first gaussian component\n",
    "    first_comp = [mu[i][0] for mu in mus_out]\n",
    "    second_comp = [mu[i][1] for mu in mus_out]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].plot(np.arange(len(first_comp)), first_comp)\n",
    "    axes[0].set_title(str(i) + \"__mu_x\", fontsize=16)\n",
    "\n",
    "    axes[1].plot(np.arange(len(second_comp)), second_comp,)\n",
    "    axes[1].set_title(str(i) + \"_mu_y\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    \n",
    "    #Get mean of the first gaussian component\n",
    "    first_comp = [covs[i][0,0] for covs in covs_out]\n",
    "    second_comp = [covs[i][0,1] for covs in covs_out]\n",
    "    third_comp = [covs[i][1,1] for covs in covs_out]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "\n",
    "    axes[0].plot(np.arange(len(first_comp)), first_comp)\n",
    "    axes[0].set_title(str(i) + \"__cov_xx\", fontsize=16)\n",
    "\n",
    "    axes[1].plot(np.arange(len(second_comp)), second_comp,)\n",
    "    axes[1].set_title(str(i) + \"_cov_xy\", fontsize=16)\n",
    "    \n",
    "    axes[2].plot(np.arange(len(third_comp)), third_comp,)\n",
    "    axes[2].set_title(str(i) + \"_cov_yy\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    weights = [w[i] for w in weights_out]\n",
    "    plt.plot(np.arange(len(weights)) , weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rpy2\n",
    "# import rpy2.robjects.packages as rpackages\n",
    "# from rpy2.robjects import numpy2ri\n",
    "# numpy2ri.activate()\n",
    "\n",
    "# c_chain = np.vstack(allocs_out)\n",
    "# c_chain=c_chain+1 # con cluster 0 altrimenti da errore perchè fa un -1 dentro l'algoritmo non so perchè\n",
    "\n",
    "# GreedyEPL = rpackages.importr('GreedyEPL')\n",
    "# best_clust_obj=GreedyEPL.MinimiseEPL(c_chain,pars = list(\"B\"))\n",
    "# best_clust=best_clust_obj[2]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slow functions\n",
    "\n",
    "def sample_uniq_vals_fullcond_Wasserstein_NW_log_slow(clusdata, mus,covs, h,bij,cov_prop_mu,cov_prop_cov):\n",
    "    \"\"\"\n",
    "    Perform one step of the Metropolis Hastings to sample a couple (mu,cov) from N*IW*Wasserstein\n",
    "    \"\"\"\n",
    "    #Starting point of MH\n",
    "    mu_old = mus[h]\n",
    "    cov_old = covs[h]\n",
    "    log_jac_cov_old = abs(bij.forward_log_det_jacobian( bij.inverse(cov_old) ) )\n",
    "    \n",
    "    # Sample the proposal using Bijector and compute jacobian term of transformation\n",
    "    mu, cov, log_jac_cov = sample_from_bijector_slow(mu_old, cov_prop_mu, cov_old, cov_prop_cov,bij)\n",
    "    jac_term = log_jac_cov - log_jac_cov_old\n",
    "    \n",
    "    # Compute acceptance rate(and return it for tuning)\n",
    "    beta = compute_beta_NW_log_slow(clusdata,mus,covs, h, mu, cov,mu_old, cov_old,jac_term)\n",
    "    beta = np.minimum(0, beta)\n",
    "    accept_rate = np.exp(beta)\n",
    "    \n",
    "    draw = tfd.Uniform().sample()\n",
    "    \n",
    "    # Select the new or old values \n",
    "    if(np.log(draw) >= beta):\n",
    "        return mu_old,cov_old,accept_rate\n",
    "\n",
    "    else:\n",
    "        return mu,cov,accept_rate\n",
    "    \n",
    "def sample_uniq_vals_prior_Wasserstein_NW_log_slow(mus,covs, h,bij,cov_prop_mu,cov_prop_cov):\n",
    "    \"\"\"\n",
    "    Perform one step of the Metropolis Hastings to sample a couple (mu,cov) from N*IW*Wasserstein\n",
    "    \"\"\"\n",
    "    # Starting point of MH\n",
    "    mu_old = mus[h]\n",
    "    cov_old = covs[h]\n",
    "    log_jac_cov_old = abs(bij.forward_log_det_jacobian( bij.inverse(cov_old) ) )\n",
    "    \n",
    "    # Sample the proposal using Bijector and compute jacobian term of transformation\n",
    "    mu, cov, log_jac_cov = sample_from_bijector_slow(mu_old, cov_prop_mu, cov_old, cov_prop_cov,bij)\n",
    "    jac_term = log_jac_cov - log_jac_cov_old\n",
    "    \n",
    "    # Compute acceptance rate(and return it for tuning)\n",
    "    beta = compute_beta_NW_prior_log_slow(mus,covs, h, mu, cov,mu_old, cov_old,jac_term)\n",
    "    beta = np.minimum(0, beta)\n",
    "    accept_rate = np.exp(beta)\n",
    "    \n",
    "    draw = tfd.Uniform().sample()\n",
    "    \n",
    "    # Select the new or old values \n",
    "    if(np.log(draw) >= beta):\n",
    "        return np.array(mu_old),np.array(cov_old),accept_rate\n",
    "\n",
    "    else:\n",
    "        return np.array(mu),np.array(cov),accept_rate\n",
    "    \n",
    "\n",
    "def sample_from_bijector_slow(mu_old, cov_prop_mu, cov_old, cov_prop_cov, bijector):\n",
    "    \"\"\"\n",
    "    Sample new mean and covariance matrix through the bijection\n",
    "    \"\"\"\n",
    "    mu = tfd.MultivariateNormalFullCovariance(mu_old, cov_prop_mu).sample()\n",
    "    \n",
    "    chol = np.linalg.cholesky(cov_old)\n",
    "    vec = bijector.inverse(chol)\n",
    "    vec_sampl = tfd.MultivariateNormalFullCovariance(vec,cov_prop_cov).sample()\n",
    "    cov_sampl = bijector.forward(vec_sampl)\n",
    "    cov = np.matmul(cov_sampl , np.transpose(cov_sampl))\n",
    "    log_jac = abs(bijector.forward_log_det_jacobian(vec_sampl))\n",
    "    \n",
    "    return mu,cov,log_jac\n",
    "\n",
    "def compute_beta_NW_log_slow(clusdata,mus,covs, h, mu, cov,mu_old, cov_old,jac_term):\n",
    "    \"\"\"\n",
    "    Compute Metropolis Hastings term, knowing the proposal is symmetric\n",
    "    The target is the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_n\n",
    "    \"\"\"\n",
    "    \n",
    "    mu_0 = np.zeros(2)\n",
    "    cov_0 = 10*np.eye(2)\n",
    "    nu_0 = 15\n",
    "    phi_0 = (nu_0 - 3)*np.eye(2)\n",
    "    \n",
    "    # NW distribution terms\n",
    "    num_0 = evaluate_NW_log_slow(mu, cov, mu_0, cov_0, phi_0, nu_0)\n",
    "    den_0 = evaluate_NW_log_slow(mu_old, cov_old , mu_0, cov_0, phi_0, nu_0)\n",
    "    \n",
    "    # Data distribution terms\n",
    "    num_1 = np.sum(tfd.MultivariateNormalFullCovariance(mu, cov).log_prob(clusdata))\n",
    "    den_1 = np.sum(tfd.MultivariateNormalFullCovariance(mu_old, cov_old).log_prob(clusdata))\n",
    "    \n",
    "    # Wasserstein Distance terms\n",
    "    sum_ = 0\n",
    "    for j in range(k):\n",
    "        if(j != h):\n",
    "            mu_j = mus[j]\n",
    "            cov_j = covs[j]\n",
    "            sum_ = sum_ + np.log(compute_Wasserstein_slow(mu , cov , mu_j , cov_j)) - np.log(compute_Wasserstein_slow(mu_old , cov_old , mu_j, cov_j))\n",
    "\n",
    "    return num_0 + num_1  - den_0 - den_1 + sum_ + jac_term\n",
    "\n",
    "def compute_beta_NW_prior_log_slow(mus,covs, h, mu, cov,mu_old, cov_old,jac_term):\n",
    "    \"\"\"\n",
    "    Compute Metropolis Hastings term, knowing the proposal is symmetric\n",
    "    The target is the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nus the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_\"s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nus the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_s the Normal Inverse Wishart with parameters mu_n, lam_n, phi_n, nu_\"n\n",
    "    \"\"\"\n",
    "    \n",
    "    mu_0 = np.zeros(2)\n",
    "    cov_0 = 10*np.eye(2)\n",
    "    nu_0 = 15\n",
    "    phi_0 = (nu_0 - 3)*np.eye(2)\n",
    "    \n",
    "    # NW distribution terms\n",
    "    num = evaluate_NW_log_slow(mu, cov, mu_0, cov_0, phi_0, nu_0)\n",
    "    den = evaluate_NW_log_slow(mu_old, cov_old , mu_0, cov_0, phi_0, nu_0)\n",
    "    \n",
    "    # Wasserstein Distance terms\n",
    "    sum_ = 0\n",
    "    for j in range(k):\n",
    "        if(j != h):\n",
    "            mu_j = mus[j]\n",
    "            cov_j = covs[j]\n",
    "            sum_ = sum_ + np.log(compute_Wasserstein_slow(mu , cov , mu_j , cov_j)) - np.log(compute_Wasserstein_slow(mu_old , cov_old , mu_j, cov_j))\n",
    "\n",
    "    return num  - den + sum_ + jac_term\n",
    "  \n",
    "def evaluate_NW_log_slow(x_mu, x_cov, mu_n, cov_n, phi_n, nu_n):\n",
    "    \"\"\"\n",
    "    Evaluate the likelihood: L[ (x_mu, x_cov) | ( mu_n, cov_n, phi_n, nu_n ) ]\n",
    "    L = Normal x Inverse Wishart\n",
    "    \"\"\"\n",
    "    W = np.linalg.inv(phi_n)\n",
    "    chol = np.linalg.cholesky(W) \n",
    "    inv_cov = np.linalg.inv(x_cov)\n",
    "\n",
    "    f1 = tfd.WishartTriL(df=nu_n, scale_tril=chol).log_prob(inv_cov)\n",
    "    f2 = tfd.MultivariateNormalFullCovariance(mu_n, cov_n).log_prob(x_mu)\n",
    "    \n",
    "    return f1 + f2\n",
    "\n",
    "def compute_Wasserstein_slow(mu_1, cov_1, mu_2, cov_2):\n",
    "    \"\"\"\n",
    "    Wasserstein distance for the Gaussian Case, already taking consideration of g() to bound Wasserstein term\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(mu_1 - mu_2, ord = 2)\n",
    "    sqrt_C2 = scipy.linalg.sqrtm(cov_2)\n",
    "    C1_sqrt_C2 = np.matmul(cov_1,sqrt_C2)\n",
    "    sqrt_C2_C1_sqrt_C2 = np.matmul(sqrt_C2,C1_sqrt_C2)\n",
    "    trace = np.trace(cov_1 + cov_2 - 2 * scipy.linalg.sqrtm(sqrt_C2_C1_sqrt_C2))\n",
    "\n",
    "    # Function g\n",
    "    return g(norm+trace)\n",
    "\n",
    "def update_weights_slow(cluster_allocs, n_clus, k, alpha):\n",
    "\n",
    "    n_by_clus = np.array([np.sum(cluster_allocs == h) for h in range(n_clus)])\n",
    "    post_params = np.ones(k) * alpha + n_by_clus\n",
    "    return tfd.Dirichlet(post_params.astype(float)).sample()\n",
    "\n",
    "def update_cluster_allocs_slow(data, weights, mus, covs):\n",
    "    #print(weights)\n",
    "    logprobs = tfd.MultivariateNormalFullCovariance(mus, covs).log_prob(data[:, np.newaxis])\n",
    "    logprobs += np.log(weights)\n",
    "    probs =  np.exp(logprobs)/(np.sum(np.exp(logprobs), axis=1))[:,None]\n",
    "    return tfd.Categorical(probs=probs, validate_args=True).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data to use for comparison\n",
    "key = rng_key\n",
    "jtfb = jtfp.bijectors\n",
    "bij = jtfb.Chain([\n",
    "        jtfb.TransformDiagonal(jtfb.Softplus()),\n",
    "        jtfb.FillTriangular()])\n",
    "\n",
    "#Starting Values\n",
    "cluster_allocs = tfd.Categorical(probs=np.ones(k) / k).sample(len(data))\n",
    "weights = jnp.ones(k)/k\n",
    "alpha = 0.1\n",
    "lam = 0.1\n",
    "mus = np.array(jtfd.MultivariateNormalFullCovariance(np.mean(np.array(data), axis=0), np.eye(d) ).sample(k,seed = key))\n",
    "covs = np.array(jtfd.WishartTriL(df=d, scale_tril=tf.linalg.cholesky(np.eye(d)) ).sample(k,seed = key))\n",
    "\n",
    "#Proposal Covariances\n",
    "cov_prop_cov = 0.02 * jnp.eye(3) #covariance matrix for the covariance sampling\n",
    "cov_prop_mu = 0.1 * jnp.eye(2)\n",
    "h = 3\n",
    "clusdata = data[cluster_allocs == h]\n",
    "\n",
    "\n",
    "%timeit sample_uniq_vals_fullcond_Wasserstein_NW_log(data,cluster_allocs, mus,covs, h,bij,cov_prop_mu,cov_prop_cov,key)\n",
    "%timeit sample_uniq_vals_fullcond_Wasserstein_NW_log_slow(clusdata, mus,covs, h,bij,cov_prop_mu,cov_prop_cov)\n",
    "%timeit compute_Wasserstein(mus[0], covs[0], mus[3], covs[3])\n",
    "%timeit compute_Wasserstein_slow(mus[0], covs[0], mus[3], covs[3])\n",
    "%timeit sample_uniq_vals_prior_Wasserstein_NW_log(mus,covs, h,bij,cov_prop_mu,cov_prop_cov,key)\n",
    "%timeit sample_uniq_vals_prior_Wasserstein_NW_log_slow(mus,covs, h,bij,cov_prop_mu,cov_prop_cov)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
