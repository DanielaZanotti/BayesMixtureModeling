{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"Create cluster NIW.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"6172caea","outputId":"81473883-8e6f-4feb-ced4-7731b33f56d6"},"source":["%%javascript\n","MathJax.Hub.Config({\n","    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n","});"],"id":"6172caea","execution_count":null,"outputs":[{"data":{"application/javascript":["MathJax.Hub.Config({\n","    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n","});\n"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"id":"adc3b3b2"},"source":["# LIBRARIES"],"id":"adc3b3b2"},{"cell_type":"code","metadata":{"id":"01714d41"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","import pandas as pd\n","import time\n","\n","from tensorflow_probability.substrates import numpy as tfp\n","tfd = tfp.distributions\n","\n","np.random.seed(0)"],"id":"01714d41","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40487117"},"source":["# LOAD DATA"],"id":"40487117"},{"cell_type":"code","metadata":{"id":"e4d95090"},"source":["sampled_perturbed = np.load('sampled_2D_two_clusters.npy')"],"id":"e4d95090","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16d7de95"},"source":["sampled_true = np.load('sampled_2D_two_clusters_true.npy')"],"id":"16d7de95","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"882a2ee6"},"source":["data = sampled_perturbed[0 : 1000]"],"id":"882a2ee6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"79aaac7d"},"source":["#DEFINING NUMBER OF CLUSTERS AND DIMENSION OF DATA\n","k = 10\n","d = 2"],"id":"79aaac7d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b5f2a8f"},"source":["# GIBBS SAMPLER"],"id":"5b5f2a8f"},{"cell_type":"code","metadata":{"id":"f2f180fb"},"source":["#FUNCTION TO RUN ONE GIBBS SAMPLER ITERATION\n","#TAKES IN INPUT VALUES OF THE PREVIOUS ITERATION AND SAMPLE THE NEW VALUES BY THE FUNCTIONS sample_uniq_vals_fullcond, update_weights and update_cluster_allocs\n","#RETURN:  -cluster_allocs: A VECTOR WITH FOR EVERY DATA POINT THE CLUSTER TO WHICH IT IS ASSIGNED\n","#         -uniq_vals: A MATRIX WITH #ROWS=CLUSTERS AND 2 COLUMNS, IN EACH COLUMN THERE IS AN ARRAY LIKE [MU[0], VAR[0,0], VAR[0,1]]\n","#         -weights: A VECTOR WITH THE WEIGHTS OF THE CLUSTERS\n","\n","def run_one_gibbs(data, cluster_allocs, uniq_vals, weights, alpha, lam):\n","\n","    n_clus = len(weights)\n","    \n","    for h in range(n_clus):\n","        clusdata = data[cluster_allocs == h]\n","        if len(clusdata) == 0:\n","            uniq_vals[h, :] = sample_uniq_vals_prior(lam)\n","        else:\n","            uniq_vals[h, :] = sample_uniq_vals_fullcond(clusdata, lam)\n","    \n","    weights = update_weights(cluster_allocs, n_clus, k, alpha)\n","    \n","    cluster_allocs = update_cluster_allocs(data, weights, uniq_vals)\n","    \n","    return cluster_allocs, uniq_vals, weights\n","\n","\n","# FUNCTION TO RUN THE ENTIRE MARKOV CHAIN\n","# TAKES IN INPUT DATA, NUMBER OF CLUSTERS, NUMBER OF ITERATIONS, BURN-IN AND THIN\n","# IT INITIALIZE THE PARAMETERS AND THEN AT EACH ITERATION CALLS THE FUNCTION run_one_gibbs\n","# AFTER THE BURN IN IT COLLECT THE PARAMETERS IN THREE LISTS\n","# RETURN THE LISTS\n","\n","def run_mcmc(data, k, niter=2500, nburn=500, thin=5):   \n","    b = time.time() # only to measure time\n","    \n","    cluster_allocs = tfd.Categorical(probs=np.ones(k) / k).sample(len(data))\n","    weights = np.ones(k)/k\n","    alpha = 0.1\n","    lam = 0.1\n","    \n","    # 4  bivariate normal e 4 matrices 2x2\n","    uniq_vals = np.dstack([\n","        tfd.MultivariateNormalFullCovariance(np.mean(np.array(data), axis=0), np.linalg.inv(np.diag(np.ones(d))/lam)).sample(k),\n","        tfd.WishartTriL(df=d, scale_tril=tf.linalg.cholesky(np.diag(np.ones(d))) ).sample(k)])\n","    \n","    allocs_out = []\n","    uniq_vals_out = []\n","    weights_out = []\n","    \n","    for i in range(niter):\n","        cluster_allocs, uniq_vals, weights = run_one_gibbs(\n","            data, cluster_allocs, uniq_vals, weights, alpha, lam)\n","        \n","        if i > nburn and i % thin == 0:\n","            allocs_out.append(cluster_allocs)\n","            uniq_vals_out.append(uniq_vals.copy())\n","            weights_out.append(weights)\n","            \n","        if i % 10 == 0:\n","            a = time.time()\n","            print(\"\\rIter {0} / {1}\".format(i+1, niter) + \" Remaining minutes: \" + str(round((a-b)*(niter-i)/(60*10) ,1)) , flush=False, end=\" \")\n","            b = time.time()\n","            \n","    return allocs_out, uniq_vals_out, weights_out"],"id":"f2f180fb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ee28687e"},"source":["# FUNCTION TO UPDATE THE CLUSTER ALLOCATIONS\n","# IT EVALUATE THE MIXTURE ON THE DATA AND COMPUTE THE NEW PROBABILITIES FOR EACH CLUSTER AND RETURN THE NEW ALLOCATIONS\n","\n","def update_cluster_allocs(data, weights, uniq_vals):\n","    \n","    logprobs = tfd.MultivariateNormalFullCovariance(uniq_vals[:,:,0], uniq_vals[:,:,1:3]).log_prob(data[:, np.newaxis])\n","    logprobs += np.log(weights)\n","    probs =  np.exp(logprobs)/np.sum(np.exp(logprobs), axis=1)[:,None]\n","    for i in range(len(probs)):\n","        if np.all(probs[i] == 0):\n","            probs[i] = np.ones(k) / k\n","    return tfd.Categorical(probs=probs, validate_args=True).sample()\n","\n","\n","# FUNCTION TO UPDATE THE WEIGHTS \n","# IT UPDATES THE PARAMETERS OF THE DIRICHLET AND RETURNS THE NEW WEIGHTS\n","def update_weights(cluster_allocs, n_clus, k, alpha):\n","\n","    n_by_clus = np.array([np.sum(cluster_allocs == h) for h in range(n_clus)])\n","    post_params = np.ones(k) * alpha + n_by_clus\n","    return tfd.Dirichlet(post_params.astype(float)).sample()\n","\n","\n","# FUNCTIONS TO SAMPLE FROM THE PRIORS AND FROM THE FULL CONDITIONALS\n","# THEY BOTH USE THE CHOLESKY FACTORIZATION FOR THE NEW COVARIANCE MATRIX AND THEN SAMPLE FROM A WISHART(COV MATRIX) AND MULTIVARIATE NORMAL(MEAN)\n","# RETURN AN ARRAY OF ARRAYS\n","def sample_uniq_vals_prior(lam):\n","    \n","    chol = tf.linalg.cholesky(np.diag(np.ones(d))) \n","    prec = tfd.WishartTriL(df=d, scale_tril=chol).sample()\n","    var = np.array(tf.linalg.inv(prec))\n","    mu = tfd.MultivariateNormalFullCovariance(np.mean(np.array(data), axis = 0), var/lam).sample()\n","    return np.array([[mu[0], var[0,0], var[0,1]],[mu[1], var[1,0], var[1,1]]])\n","    \n","\n","def sample_uniq_vals_fullcond(clusdata, lam):\n","    \n","    n=len(clusdata)\n","    W0 = np.diag(np.ones(d))\n","    C = (n-1)*np.cov(clusdata, rowvar = False)\n","    D = lam*n/(lam+n)*np.dot(np.mean(clusdata, axis = 0)-np.mean(data, axis = 0),np.mean(clusdata, axis = 0)-np.mean(data, axis = 0))\n","    W = tf.linalg.inv(W0 + C + D)\n","    chol = tf.linalg.cholesky(W) \n","    prec = tfd.WishartTriL(df=d+n, scale_tril=chol).sample()\n","    var = np.array(tf.linalg.inv(prec))\n","    mu = tfd.MultivariateNormalFullCovariance((lam * np.mean(np.array(data), axis = 0)+n*np.mean(clusdata, axis = 0))/(lam+n), var/(lam+n)).sample()\n","    return np.array([[mu[0], var[0,0], var[0,1]],[mu[1], var[1,0], var[1,1]]])\n"],"id":"ee28687e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"551af5ee"},"source":["#RUNNING THE MCMC\n","allocs_out, uniq_vals_out, weights_out = run_mcmc(data, k)"],"id":"551af5ee","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5d7c5876"},"source":["# PLOT OF THE CLUSTERS"],"id":"5d7c5876"},{"cell_type":"code","metadata":{"id":"ed6a3e6a"},"source":["#View the number of points inside each cluster at the final iteration\n","print(pd.Series(allocs_out[-1]).value_counts())\n","\n","x, y = np.unique(allocs_out[-1], return_counts=True)\n","\n","\n","plt.bar(x, y)\n"],"id":"ed6a3e6a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdb6d8af"},"source":["#scatterplot of clusters\n","for h in range(k):\n","    currd = data[allocs_out[-1] == h]\n","    sns.scatterplot(x=[elem[0] for elem in currd], y=[elem[1] for elem in currd])"],"id":"bdb6d8af","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgJTcwJZ09hF"},"source":["#scatterplot of clusters with marginal distributions\n","data1=pd.DataFrame(data)\n","data1[\"allocs\"]=allocs_out[-1]\n","sns.jointplot(data=data1,x=data1[0],y=data1[1], hue=\"allocs\",palette=sns.color_palette(\"hls\", 4))"],"id":"rgJTcwJZ09hF","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8c70ecf8"},"source":["#MEDIE NEI CLUSTER DELL'ULTIMA ITERAZIONE\n","M = [[np.mean(data[allocs_out[-1] == h][:,0]), np.mean(data[allocs_out[-1] == h][:,1])] for h in [5,8,9]]\n","M=pd.DataFrame(M)\n"],"id":"8c70ecf8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8ffa441"},"source":["# TRACE PLOTS"],"id":"c8ffa441"},{"cell_type":"code","metadata":{"id":"bc8d14ae"},"source":["means = [val[:,:,0] for val in uniq_vals_out]\n","for i in range(len(weights_out[0])):\n","    \n","\n","    #Get mean of the first gaussian component\n","    first_comp = [mean[i,:] for mean in means]\n","\n","    #Get mu_1 and mu_2 from the first component (multivariate) mean\n","    first_comp_first = [first_comp_mean[0] for first_comp_mean in first_comp]\n","    first_comp_second = [first_comp_mean[1] for first_comp_mean in first_comp]\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n","\n","    axes[0].plot(np.arange(len(first_comp_first)), first_comp_first)\n","    axes[0].set_title(str(i) + \"_comp_first\", fontsize=16)\n","\n","    axes[1].plot(np.arange(len(first_comp_second)), first_comp_second,)\n","    axes[1].set_title(str(i) + \"_comp_second\", fontsize=16)\n","\n","    plt.show()"],"id":"bc8d14ae","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02a90613"},"source":["first_weight = [weight[0] for weight in weights_out]\n","second_weight = [weight[1] for weight in weights_out]\n","#third_weight = [weight[2] for weight in weights_out]\n","#fourth_weight = [weight[3] for weight in weights_out]\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","\n","ax.plot(np.arange(len(first_weight)), first_weight)\n","ax.plot(np.arange(len(second_weight)), second_weight)\n","#ax.plot(np.arange(len(third_weight)), third_weight)\n","#ax.plot(np.arange(len(fourth_weight)), fourth_weight)\n","ax.set_title(\"weights\", fontsize=16)\n","\n","\n","plt.show()"],"id":"02a90613","execution_count":null,"outputs":[]}]}