{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913a12e0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34b7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import invwishart\n",
    "\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dac308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.load('true.npy')\n",
    "perturbed = np.load('perturbed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8cdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9abc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602f4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_gibbs(data, cluster_allocs, uniq_vals, weights):\n",
    "    \"\"\"\n",
    "    Runs one iteration of the gibbs sampler\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of floats, contains the observations\n",
    "    cluster_allocs: a numpy array of integers, contains the c_i's(same length of data)\n",
    "    uniq_vals: a numpy array of size [K, 2], contains (mean, variance) for every component\n",
    "    weights: a numpy array of size [K], contains the weights of the components\n",
    "    \"\"\"\n",
    "    \n",
    "    n_clus = len(weights)\n",
    "    \n",
    "    for h in range(n_clus):\n",
    "        clusdata = data[cluster_allocs == h]\n",
    "        if len(clusdata) == 0:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_prior()\n",
    "        else:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_fullcond(clusdata)\n",
    "    \n",
    "    cluster_allocs = update_cluster_allocs(data, weights, uniq_vals)\n",
    "    \n",
    "    weights = update_weights(cluster_allocs, n_clus)\n",
    "    \n",
    "    return cluster_allocs, uniq_vals, weights\n",
    "\n",
    "\n",
    "\n",
    "def run_mcmc(data, k, niter=7500, nburn=2500, thin=5):\n",
    "    cluster_allocs = tfd.Categorical(probs=np.ones(k) / k).sample(len(data))\n",
    "    weights = np.ones(k) / k\n",
    "    \n",
    "    ### CAPIRE LA INVERSE WISHART\n",
    "    uniq_vals = np.dstack([\n",
    "        tfd.MultivariateNormalFullCovariance(np.mean(np.array(data)), np.diag(20*np.ones(d))).sample(k),\n",
    "        invwishart.rvs(df = d+1, scale = np.diag(2*np.ones(d)),size=k)])\n",
    "    \n",
    "    allocs_out = []\n",
    "    uniq_vals_out = []\n",
    "    weights_out = []\n",
    "    \n",
    "    for i in range(niter):\n",
    "        cluster_allocs, uniq_vals, weights = run_one_gibbs(\n",
    "            data, cluster_allocs, uniq_vals, weights)\n",
    "        \n",
    "        if i > nburn and i % thin == 0:\n",
    "            allocs_out.append(cluster_allocs)\n",
    "            uniq_vals_out.append(uniq_vals)\n",
    "            weights_out.append(weights)\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            print(\"\\rIter {0} / {1}\".format(i+1, niter), flush=True, end=\" \")\n",
    "            \n",
    "    return allocs_out, uniq_vals_out, weights_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4150060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "def update_cluster_allocs(data, weights, uniq_vals):\n",
    "    \n",
    "    logprobs = tfd.Normal(\n",
    "        uniq_vals[:, 0], np.sqrt(uniq_vals[:, 1])).log_prob(data[:, np.newaxis])\n",
    "    logprobs += np.log(weights)\n",
    "    probs =  np.exp(logprobs)/np.sum(np.exp(logprobs))\n",
    "    return tfd.Categorical(probs=probs).sample()\n",
    "\n",
    "\n",
    "def update_weights(cluster_allocs, n_clus, k):\n",
    "\n",
    "    n_by_clus = np.array([np.sum(cluster_allocs == h) for h in range(n_clus)])\n",
    "    post_params = np.ones(k) / k + n_by_clus\n",
    "    return tfd.Dirichlet(post_params.astype(float)).sample()\n",
    "\n",
    "\n",
    "def sample_uniq_vals_prior():\n",
    "    \n",
    "    var = invwishart.rvs(df = d+1, scale = np.diag(2*np.ones(d)),size=1)\n",
    "    mu = tfd.MultivariateNormalFullCovariance(np.mean(np.array(data)), np.diag(20*np.ones(d))).sample()\n",
    "    return np.array([mu, var])\n",
    "    \n",
    "\n",
    "def sample_uniq_vals_fullcond(clusdata):\n",
    "    \n",
    "    n=len(clusdata)\n",
    "    \n",
    "    var = invwishart.rvs(df = d+1+n, scale = np.diag(2*np.ones(d))+n*np.var(clusdata)+(n/1+n)*np.dot(np.mean(clusdata)-np.mean(data),np.mean(clusdata)-np.mean(data))\n",
    "                         ,size=1)\n",
    "    mu = tfd.MultivariateNormalFullCovariance((np.mean(np.array(data))+n*np.mean(clusdata))/(1+n), np.diag(20*np.ones(d))/(1+n)).sample()\n",
    "    return np.array([mu, var], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26710ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,2) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_5416/1481455572.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mallocs_out\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muniq_vals_out\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights_out\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_mcmc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_5416/1591891712.py\u001B[0m in \u001B[0;36mrun_mcmc\u001B[1;34m(data, k, niter, nburn, thin)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mniter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m         cluster_allocs, uniq_vals, weights = run_one_gibbs(\n\u001B[0m\u001B[0;32m     44\u001B[0m             data, cluster_allocs, uniq_vals, weights)\n\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_5416/1591891712.py\u001B[0m in \u001B[0;36mrun_one_gibbs\u001B[1;34m(data, cluster_allocs, uniq_vals, weights)\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0muniq_vals\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msample_uniq_vals_prior\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m             \u001B[0muniq_vals\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msample_uniq_vals_fullcond\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclusdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m     \u001B[0mcluster_allocs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mupdate_cluster_allocs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muniq_vals\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_5416/1303832527.py\u001B[0m in \u001B[0;36msample_uniq_vals_fullcond\u001B[1;34m(clusdata)\u001B[0m\n\u001B[0;32m     33\u001B[0m                          ,size=1)\n\u001B[0;32m     34\u001B[0m     \u001B[0mmu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtfd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMultivariateNormalFullCovariance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclusdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvar\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mobject\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m: could not broadcast input array from shape (2,2) into shape (2,)"
     ]
    }
   ],
   "source": [
    "allocs_out, uniq_vals_out, weights_out = run_mcmc(data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1)\n",
    "for h in range(k):\n",
    "    currd = data[allocs_out[-1] == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.001 * (h+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}